Reddit Data Analysis Project
============================

This project contains Apache Airflow DAGs for scraping, processing, and analyzing Reddit data, subreddit. The analysis leverages DuckDB for storage and querying, and integrates LLM-based sentiment and topic analysis using the Airflow AI SDK with Ollama.

Project Structure
-----------------

- **dags/**: Contains all Airflow DAGs for scraping, loading, and analyzing Reddit data.
    - `scrape_reddit_and_load.py`: Scrapes posts and comments from Reddit, saves to CSV, and loads into DuckDB.
    - `load_and_analyze.py`: Loads Reddit data from DuckDB, performs LLM-based sentiment and topic analysis, and stores results back in DuckDB.
- **data/**: Contains CSV and DuckDB files generated by the DAGs.
- **models.py**: Contains configuration and data models used by the DAGs.
- **requirements.txt**: Python dependencies for the project.
- **Dockerfile**: Containerizes the Airflow environment for local development.

DAGs Overview
-------------

### 1. `scrape_reddit_and_load.py`
- Scrapes posts and comments from the "eb_1a" subreddit using the Reddit API (PRAW).
- Saves raw data to CSV files (both in-container and host-accessible locations).
- Loads the data into a DuckDB database for efficient querying and downstream analysis.
- Implements checkpointing and rate limiting to avoid data loss and API throttling.

### 2. `load_and_analyze.py`
- Loads Reddit data from DuckDB.
- Uses the Airflow AI SDK with Ollama (LLM) to perform sentiment and topic analysis on Reddit conversations.
- Stores analysis results (sentiment, topics, insights) back into DuckDB.
- Produces summary statistics and insights about the Reddit community.
- Handles dependency checks and table initialization automatically.

How to Run Locally
------------------

1. **Install dependencies**:
   - Add required Python packages to `requirements.txt` and OS packages to `packages.txt` if needed.
2. **Start Airflow**:
   - Run `astro dev start` to spin up the Airflow environment using Docker.
   - Access the Airflow UI at [http://localhost:8080/](http://localhost:8080/).
3. **Configure Reddit API credentials**:
   - Set your Reddit API credentials in a `.env` file or as environment variables (see `models.py` for required fields).
4. **Trigger the DAGs**:
   - Use the Airflow UI to trigger `reddit_scraper_dag` to scrape and load data.
   - Then trigger `reddit_analyzer_dag` to analyze and store results.

Outputs
-------
- **CSV files**: Raw Reddit data in `data/eb1a_threads_data.csv`.
- **DuckDB database**: Structured Reddit data and analysis results in `data/eb1a_threads_data.duckdb`.
- **Analysis reports**: Sentiment, topic, and insights summaries stored in DuckDB and available for further analysis.

Contact
-------
For questions or issues, please open an issue in this repository or contact the project maintainer.
